2025-11-24 07:23:33,440 - INFO - Training configuration: gpt2_adamwspam_ramla_stepRAMLAKV0
2025-11-24 07:23:33,441 - INFO - DDP not enabled in config - using single-GPU mode
2025-11-24 07:23:33,441 - INFO - Starting training with GPU monitoring: gpt2_adamwspam_ramla_stepRAMLAKV0
2025-11-24 07:23:33,487 - INFO - Started simple GPU monitoring - stats will be saved to test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/gpu_stats_gpt2_adamwspam_ramla_stepRAMLAKV0_20251124_072333.json
2025-11-24 07:23:33,487 - INFO - Command: /home/mcgrof/envs/w7900-ml/bin/python3 /data/knlp/gpt2/train.py --output-dir test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0 --optimizer adamwspam --dataset finewebedu --data-dir ./gpt2/data --block-size 1024 --flash-attention --warmup-steps 2000 --eval-interval 50 --eval-samples 200 --max-iters 10000 --max-time 7200 --decay-lr --min-lr 6e-5 --learning-rate 6e-4 --pruning-method none --spam-theta 50.0 --spam-interval 1000 --spam-warmup-steps 1000 --spam-enable-clip --architecture ramla --ra-step RAMLAKV0 --run-lm-eval --lm-eval-tasks hellaswag --json-output /data/knlp/test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/training_metrics.json --tracker wandb,trackio --tracker-project gpt2-kvsplice-ablation-w7900-mla-fixed --tracker-run-name gpt2_adamwspam_ramla_stepRAMLAKV0
Hyperparams: AUTO mode - GPU: AMD Radeon Pro W7900 (48.3GB total, 48.3GB free) √ó 2, model=gpt2, compile=ON ‚Üí batch=32, grad_acc=16 (effective=1024, target=1024)
Compile: Enabled (GPU 'AMD Radeon Pro W7900' has good torch.compile support)
Running RAMLA trainer (step RAMLAKV0)
Step RAMLAKV0: ramlakv architecture, standard LR (6.0e-04)
* Trackio project initialized: gpt2-kvsplice-ablation-w7900-mla-fixed
* Trackio metrics logged to: /home/mcgrof/.cache/huggingface/trackio
* View dashboard by running in your terminal:
[1m[38;5;208mtrackio show --project "gpt2-kvsplice-ablation-w7900-mla-fixed"[0m
* or by running in Python: trackio.show(project="gpt2-kvsplice-ablation-w7900-mla-fixed")
* Created new run: gpt2_adamwspam_ramla_stepRAMLAKV0
Initialized Trackio tracking for project: gpt2-kvsplice-ablation-w7900-mla-fixed
wandb: Currently logged in as: mcgrof (mcgrof-citizen) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /data/knlp/wandb/run-20251124_072336-65ox5tqe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gpt2_adamwspam_ramla_stepRAMLAKV0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-w7900-mla-fixed
wandb: üöÄ View run at https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-w7900-mla-fixed/runs/65ox5tqe
wandb: Initializing weave.
[36m[1mweave[0m: Logged in as Weights & Biases user: mcgrof.
[36m[1mweave[0m: View Weave data at https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-w7900-mla-fixed/weave
Initialized WandB tracking for project: gpt2-kvsplice-ablation-w7900-mla-fixed
Initializing GPT-2 model: gpt2
Number of parameters: 123.69M
Setting up adamwspam optimizer...
Weight decay: 0.1
Created RAMLAKV_GPT
  Compression: {'d_latent': 256, 'd_compressed': 128, 'compression_ratio': 0.5, 'cache_reduction': '50.0%'}
Number of parameters: 113.44M
Created optimizer with LR=6.0e-04

Starting training...
Parameters: 123.69M
Device: cuda, dtype: bfloat16
Batch size: 32, Gradient accumulation: 16
Effective batch size: 512
Save checkpoint: True, Output: test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0
--------------------------------------------------
Iter     0 | loss 1.1018 | ppl    3.01 | lr 0.00e+00 | sparsity 0.0% | 2203.6ms/iter

Eval @ iter 0: train 11.0164, val 11.0157, ppl 60822.82
Saved checkpoint at iteration 0
Iter    10 | loss 10.9635 | ppl 57729.14 | lr 3.00e-06 | sparsity 0.0% | 44314.0ms/iter
Iter    20 | loss 10.6576 | ppl 42515.19 | lr 6.00e-06 | sparsity 0.0% | 22527.6ms/iter
Iter    30 | loss 10.1821 | ppl 26424.83 | lr 9.00e-06 | sparsity 0.0% | 22527.3ms/iter
Iter    40 | loss 9.7666 | ppl 17441.08 | lr 1.20e-05 | sparsity 0.0% | 22529.0ms/iter
Iter    50 | loss 9.4814 | ppl 13113.46 | lr 1.50e-05 | sparsity 0.0% | 22474.8ms/iter

Eval @ iter 50: train 9.3624, val 9.3516, ppl 11516.87
Iter    60 | loss 9.2711 | ppl 10626.18 | lr 1.80e-05 | sparsity 0.0% | 44212.9ms/iter
Iter    70 | loss 9.0590 | ppl 8595.54 | lr 2.10e-05 | sparsity 0.0% | 22497.9ms/iter
Iter    80 | loss 8.8385 | ppl 6894.73 | lr 2.40e-05 | sparsity 0.0% | 22510.5ms/iter
Iter    90 | loss 8.6108 | ppl 5490.60 | lr 2.70e-05 | sparsity 0.0% | 22503.0ms/iter
Iter   100 | loss 8.3886 | ppl 4396.51 | lr 3.00e-05 | sparsity 0.0% | 22503.0ms/iter

Eval @ iter 100: train 8.2633, val 8.2342, ppl 3767.48
Iter   110 | loss 8.1728 | ppl 3543.40 | lr 3.30e-05 | sparsity 0.0% | 44220.9ms/iter
Iter   120 | loss 7.9561 | ppl 2852.94 | lr 3.60e-05 | sparsity 0.0% | 22494.7ms/iter
Iter   130 | loss 7.7518 | ppl 2325.85 | lr 3.90e-05 | sparsity 0.0% | 22503.3ms/iter
Iter   140 | loss 7.5692 | ppl 1937.58 | lr 4.20e-05 | sparsity 0.0% | 22487.7ms/iter
Iter   150 | loss 7.4135 | ppl 1658.29 | lr 4.50e-05 | sparsity 0.0% | 22494.3ms/iter

Eval @ iter 150: train 7.3389, val 7.2854, ppl 1458.84
Iter   160 | loss 7.2773 | ppl 1447.05 | lr 4.80e-05 | sparsity 0.0% | 44222.2ms/iter
Iter   170 | loss 7.1735 | ppl 1304.40 | lr 5.10e-05 | sparsity 0.0% | 22494.6ms/iter
Iter   180 | loss 7.0660 | ppl 1171.43 | lr 5.40e-05 | sparsity 0.0% | 22487.4ms/iter
Iter   190 | loss 6.9788 | ppl 1073.61 | lr 5.70e-05 | sparsity 0.0% | 22507.6ms/iter
Iter   200 | loss 6.8932 | ppl  985.56 | lr 6.00e-05 | sparsity 0.0% | 22532.2ms/iter

Eval @ iter 200: train 6.8505, val 6.8039, ppl 901.39
Iter   210 | loss 6.8252 | ppl  920.78 | lr 6.30e-05 | sparsity 0.0% | 44316.5ms/iter
Iter   220 | loss 6.7419 | ppl  847.17 | lr 6.60e-05 | sparsity 0.0% | 22531.3ms/iter
Iter   230 | loss 6.6860 | ppl  801.15 | lr 6.90e-05 | sparsity 0.0% | 22542.9ms/iter
Iter   240 | loss 6.6300 | ppl  757.49 | lr 7.20e-05 | sparsity 0.0% | 22541.7ms/iter
Iter   250 | loss 6.5871 | ppl  725.69 | lr 7.50e-05 | sparsity 0.0% | 22585.1ms/iter

Eval @ iter 250: train 6.5841, val 6.5268, ppl 683.24
Iter   260 | loss 6.5823 | ppl  722.19 | lr 7.80e-05 | sparsity 0.0% | 44414.0ms/iter

Reached max training time of 7200s (7204.4s elapsed)

Training complete! Total time: 120.07 minutes
Best validation perplexity achieved: 683.24
Metrics saved to /data/knlp/test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/training_metrics_stepV0.json
Saved final model: test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/final_model_stepV0.pt

--- KVSplice Compression Metrics ---
  Compression ratio: 0.5
  Memory reduction: 50.0%
  Avg reconstruction error: 0.534166
  Reciprocal layers: 6
  Standard layers: 6

--- Fisher Information Metrics ---
  layer0 eigmax_mean: 0.033318
  layer6 eigmax_mean: 0.034388
  layer11 eigmax_mean: 0.007816

--- KV Cache Memory ---
  Cache type: kvsplice
  seq=512: 1.5 MB (92% savings)
  seq=1024: 3.0 MB (92% savings)
  seq=2048: 6.0 MB (92% savings)
  seq=4096: 12.0 MB (92% savings)

--- lm-eval Benchmarks ---
Running lm-eval with limit=100 samples per task
Overwriting default num_fewshot of hellaswag from None to 0

  0%|          | 0/100 [00:00<?, ?it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 10588.47it/s]
hellaswag/acc,none: 0.2600
hellaswag/acc_stderr,none: 0.0441
hellaswag/acc_norm,none: 0.3000
hellaswag/acc_norm_stderr,none: 0.0461

--- Text Generation Samples ---

Prompt: The meaning of life is
Output: The meaning of life is not have developed.
How to be used to be in the same time of the body of a.
-in was in the end from the the two is that the body.
The first year, so they are a ‚Äúthe-...

Prompt: In a shocking discovery, scientists found that
Output: In a shocking discovery, scientists found that are given that the time.
The first to the same time are the most of this. But if the way that were not have. The same on the one, are.
What is a, and the...

Prompt: Once upon a time in a land far away,
Output: Once upon a time in a land far away, and to the country.
‚Äôt have a ‚ÄúWhen you‚Äôs a new, which is only about the the ‚ÄúWhat is more information will know,000 is in the last in the U.
This might be the...

Prompt: The best way to learn programming is
Output: The best way to learn programming is an important to the
It‚Äù in the
-7
- The
-
-
-7
-
-
- This
- This
-
-
-
-
-
-
-
- You...
* Run finished. Uploading logs to Trackio (please wait...)
Trackio tracking finished
wandb: uploading artifact run-65ox5tqe-text_samples; updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:           final/best_val_loss ‚ñÅ
wandb:     final/best_val_perplexity ‚ñÅ
wandb:       fisher/layer0/cond_mean ‚ñÑ‚ñÜ‚ñá‚ñÅ‚ñà‚ñÑ‚ñÖ
wandb:        fisher/layer0/cond_std ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÇ‚ñÑ
wandb:      fisher/layer0/decay_mean ‚ñÖ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÅ‚ñÖ
wandb:     fisher/layer0/eigmax_mean ‚ñÜ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÑ‚ñÑ
wandb:      fisher/layer0/eigmax_std ‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÑ
wandb: fisher/layer0/energy_r16_mean ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÉ
wandb:  fisher/layer0/energy_r8_mean ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñÉ
wandb:      fisher/layer0/head0/cond ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÅ
wandb:                          +570 ...
wandb: 
wandb: Run summary:
wandb:           final/best_val_loss 6.52684
wandb:     final/best_val_perplexity 683.23624
wandb:       fisher/layer0/cond_mean 2828718.72917
wandb:        fisher/layer0/cond_std 859018.47438
wandb:      fisher/layer0/decay_mean 1.40041
wandb:     fisher/layer0/eigmax_mean 0.03332
wandb:      fisher/layer0/eigmax_std 0.0068
wandb: fisher/layer0/energy_r16_mean 0.361
wandb:  fisher/layer0/energy_r8_mean 0.21344
wandb:      fisher/layer0/head0/cond 1398405.125
wandb:                          +570 ...
wandb: 
wandb: üöÄ View run gpt2_adamwspam_ramla_stepRAMLAKV0 at: https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-w7900-mla-fixed/runs/65ox5tqe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-w7900-mla-fixed
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251124_072336-65ox5tqe/logs
W&B tracking finished

Training complete!
2025-11-24 09:24:03,065 - INFO - Training completed successfully
2025-11-24 09:24:03,391 - INFO - Simple GPU monitoring completed
2025-11-24 09:24:03,947 - INFO - GPU performance graphs generated: test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/gpu_stats_gpt2_adamwspam_ramla_stepRAMLAKV0_20251124_072333_plot.png
2025-11-24 09:24:03,948 - INFO - GPU stats saved to: test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/gpu_stats_gpt2_adamwspam_ramla_stepRAMLAKV0_20251124_072333.json
2025-11-24 09:24:03,948 - INFO - Generating performance graphs...
2025-11-24 09:24:04,501 - INFO - GPU performance graphs saved to test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/gpu_stats_gpt2_adamwspam_ramla_stepRAMLAKV0_20251124_072333_plot.png
2025-11-24 09:24:04,501 - INFO - Performance graphs saved to: test_matrix_results_20251123_231956/gpt2_adamwspam_ramla_stepRAMLAKV0/gpu_stats_gpt2_adamwspam_ramla_stepRAMLAKV0_20251124_072333_plot.png
2025-11-24 09:24:04,502 - INFO - Training with monitoring completed successfully!
