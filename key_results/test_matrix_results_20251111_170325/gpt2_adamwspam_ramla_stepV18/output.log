/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-11 23:09:51,602 - INFO - Training configuration: gpt2_adamwspam_ramla_stepV18
2025-11-11 23:09:51,602 - INFO - Ablation mode enabled - using train_ra_mla.py
2025-11-11 23:09:51,603 - INFO - DDP not enabled in config - using single-GPU mode
2025-11-11 23:09:51,603 - INFO - Starting training with GPU monitoring: gpt2_adamwspam_ramla_stepV18
2025-11-11 23:09:51,656 - INFO - Started simple GPU monitoring - stats will be saved to test_matrix_results_20251111_170325/gpt2_adamwspam_ramla_stepV18/gpu_stats_gpt2_adamwspam_ramla_stepV18_20251111_230951.json
2025-11-11 23:09:51,656 - INFO - Command: /opt/conda/envs/pytorch/bin/python3 /opt/dlami/nvme/AdamWPrune/gpt2/train_ra_mla.py --optimizer adamwspam --dataset finewebedu --batch-size 8 --block-size 1024 --gradient-accumulation 8 --compile --warmup-steps 200 --eval-interval 100 --eval-samples 200 --max-iters 10000 --max-time 7200 --decay-lr --min-lr 6e-5 --learning-rate 6e-4 --pruning-method none --spam-theta 50.0 --spam-interval 1000 --spam-warmup-steps 1000 --spam-enable-clip --ra-mla-ablation-step V18 --json-output /opt/dlami/nvme/AdamWPrune/test_matrix_results_20251111_170325/gpt2_adamwspam_ramla_stepV18/training_metrics.json --tracker wandb,trackio --tracker-project gpt2-kv-pruning --tracker-run-name gpt2_adamwspam_ramla_stepV18
/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Time-based training enabled: max 7200 seconds (2.00 hours)
Using device: cuda (NVIDIA A10G)
  Enabled TensorFloat32 matmul precision (WMMA/Tensor Cores)
  torch.compile enabled with scalar output capture
Creating GPT-2 model: gpt2
Number of parameters: 123.69M
======================================================================
Applying R-MLP standalone (baseline GPT-2 attention):
  MLP R_ff value:       1152
  MLP expansion:        4.0x
  Split: D_ff_std=1920, R_ff=1152 (ratio 1.67:1)
  R-MLP features:       basic (no mixer/gates)
======================================================================
Patching GPT-2 with KV cache pruning (learned, recency=64)...
  Layer 0: Standard Attention â†’ KV-Pruned Attention
  Layer 1: Standard Attention â†’ KV-Pruned Attention
  Layer 2: Standard Attention â†’ KV-Pruned Attention
  Layer 3: Standard Attention â†’ KV-Pruned Attention
  Layer 4: Standard Attention â†’ KV-Pruned Attention
  Layer 5: Standard Attention â†’ KV-Pruned Attention
  Layer 6: Standard Attention â†’ KV-Pruned Attention
  Layer 7: Standard Attention â†’ KV-Pruned Attention
  Layer 8: Standard Attention â†’ KV-Pruned Attention
  Layer 9: Standard Attention â†’ KV-Pruned Attention
  Layer 10: Standard Attention â†’ KV-Pruned Attention
  Layer 11: Standard Attention â†’ KV-Pruned Attention
Successfully patched 12 layers with KV-pruned attention
Patching GPT-2 with R-MLP (R_ff=1152, expansion=4.0)...
  Layer 0: Standard MLP â†’ R-MLP
  Layer 1: Standard MLP â†’ R-MLP
  Layer 2: Standard MLP â†’ R-MLP
  Layer 3: Standard MLP â†’ R-MLP
  Layer 4: Standard MLP â†’ R-MLP
  Layer 5: Standard MLP â†’ R-MLP
  Layer 6: Standard MLP â†’ R-MLP
  Layer 7: Standard MLP â†’ R-MLP
  Layer 8: Standard MLP â†’ R-MLP
  Layer 9: Standard MLP â†’ R-MLP
  Layer 10: Standard MLP â†’ R-MLP
  Layer 11: Standard MLP â†’ R-MLP
Successfully patched 12 layers with R-MLP
Using variance-guided activation (check every 50 steps)
  Target variance: 0.0200, window: 100 losses
Compiling model with torch.compile()...
/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Model parameters: 124.39M
Optimizer: adamwspam, LR: 0.0006, weight_decay: 0.1

Initializing experiment tracking: wandb, trackio
  Project: gpt2-kv-pruning
  Run: gpt2_adamwspam_ramla_stepV18
* Trackio project initialized: gpt2-kv-pruning
* Trackio metrics logged to: /home/mcgrof/.cache/huggingface/trackio
* View dashboard by running in your terminal:
[1m[38;5;208mtrackio show --project "gpt2-kv-pruning"[0m
* or by running in Python: trackio.show(project="gpt2-kv-pruning")
* Created new run: gpt2_adamwspam_ramla_stepV18
  âœ“ Trackio initialized
  âœ“ Logged .config path to Trackio
wandb: Currently logged in as: mcgrof (mcgrof-citizen) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /opt/dlami/nvme/AdamWPrune/gpt2/wandb/run-20251111_231006-8aeiii5q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gpt2_adamwspam_ramla_stepV18
wandb: â­ï¸ View project at https://wandb.ai/mcgrof-citizen/gpt2-kv-pruning
wandb: ğŸš€ View run at https://wandb.ai/mcgrof-citizen/gpt2-kv-pruning/runs/8aeiii5q
wandb: Initializing weave.
[36m[1mweave[0m: wandb version 0.23.0 is available!  To upgrade, please run:
[36m[1mweave[0m:  $ pip install wandb --upgrade
[36m[1mweave[0m: weave version 0.52.16 is available!  To upgrade, please run:
[36m[1mweave[0m:  $ pip install weave --upgrade
[36m[1mweave[0m: Logged in as Weights & Biases user: mcgrof.
[36m[1mweave[0m: View Weave data at https://wandb.ai/mcgrof-citizen/gpt2-kv-pruning/weave
  âœ“ WandB initialized
  âœ“ Uploaded .config to WandB

Starting training for 10000 iterations...

============================================================
TRAINING CONFIGURATION
============================================================
Batch size: 8, gradient accumulation: 8
Effective batch size: 64
Optimizer: adamwspam
Pruning: None
RATIO Ablation: Step V18 - Step V18
============================================================


======================================================================
INFERENCE SCALING LAW ANALYSIS
======================================================================

Model Configuration:
  Layers: 12
  d_model: 768
  MLP dim: 3072
  MLP:Attn ratio: 4.00
  Parameters: 124.4M (0.12GB)

Training Efficiency: MLP:Attention Width Ratio
==============================================

  Val Loss â”‚                                                          Â·Â·
           â”‚                                                        Â·Â·  
           â”‚                                                      Â·Â·    
           â”‚                                                    Â·Â·      
           â”‚                                                  Â·Â·        
           â”‚                                               Â·Â·Â·          
           â”‚                                             Â·Â·             
           â”‚Â·Â·                                       Â·Â·Â·Â·               
           â”‚  Â·Â·Â·Â·                                Â·Â·Â·                   
           â”‚      Â·Â·Â·Â·                       Â·Â·Â·Â·Â·                      
           â”‚          Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·                           
    (good) â”‚                     â˜…                                      
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            MLP:Attention Width Ratio
            0.5                         1.4â˜…                         3.0

  â˜… = Optimal    â— = Current configuration

  Current value (4.00) is outside plot range [0.5, 3.0]

Inference Efficiency: Depth vs Latency (0.1GB param budget)
===========================================================

   Latency â”‚                                                          Â·Â·
           â”‚                                                        Â·Â·  
           â”‚                                                      Â·Â·    
           â”‚                                                    Â·Â·      
           â”‚                                                  Â·Â·        
           â”‚                                               Â·Â·Â·          
           â”‚                                            Â·Â·Â·             
           â”‚Â·Â·                                       Â·Â·Â·                
           â”‚  Â·Â·â—                                 Â·Â·Â·                   
           â”‚     Â·Â·Â·Â·Â·                       Â·Â·Â·Â·Â·                      
           â”‚          Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·                           
    (good) â”‚                     â˜…                                      
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            Transformer Depth (layers)
            8.0                         28.0â˜…                       64.0

  â˜… = Optimal    â— = Current configuration

  Assessment: âš  Moderate - Consider adjusting towards optimal
  Current: 12.00 | Optimal: 28.00
  Suggestion: Increase transformer depth (layers) towards 28.00

======================================================================
Note: Curves based on inference scaling law research
      Optimal values may vary based on specific use case
======================================================================

Iter      0 | train loss 10.9596 | val loss 10.9582 | lr 0.00e+00 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Saved best checkpoint (val_loss=10.9582)
Iter      0 | loss 10.9645 | time 4077.8ms | lr 0.00e+00 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     10 | loss 9.9786 | time 2575.5ms | lr 3.00e-05 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     20 | loss 9.3519 | time 2576.1ms | lr 6.00e-05 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     30 | loss 8.7798 | time 2576.0ms | lr 9.00e-05 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     40 | loss 8.0921 | time 2577.4ms | lr 1.20e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     50 | loss 7.7200 | time 2575.6ms | lr 1.50e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     60 | loss 7.4533 | time 2576.7ms | lr 1.80e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     70 | loss 7.3278 | time 2576.4ms | lr 2.10e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     80 | loss 7.2036 | time 2576.7ms | lr 2.40e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter     90 | loss 7.0312 | time 2577.1ms | lr 2.70e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter    100 | train loss 6.9356 | val loss 6.8867 | lr 3.00e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.000
Saved best checkpoint (val_loss=6.8867)
Iter    100 | loss 6.8753 | time 2399.4ms | lr 3.00e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.000
Iter    110 | loss 6.8193 | time 2575.6ms | lr 3.30e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.000
Iter    120 | loss 6.7289 | time 2576.1ms | lr 3.60e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.000
Iter    130 | loss 6.7218 | time 2576.5ms | lr 3.90e-04 | RMLP_w_std 0.898 | RMLP_w_rec 0.000
Iter    140 | loss 6.6089 | time 2577.6ms | lr 4.20e-04 | RMLP_w_std 0.898 | RMLP_w_rec 0.000
Iter    150 | loss 6.5847 | time 2576.9ms | lr 4.50e-04 | RMLP_w_std 0.898 | RMLP_w_rec 0.000
Iter    160 | loss 6.5308 | time 2577.0ms | lr 4.80e-04 | RMLP_w_std 0.898 | RMLP_w_rec 0.000
Iter    170 | loss 6.5529 | time 2575.7ms | lr 5.10e-04 | RMLP_w_std 0.898 | RMLP_w_rec 0.000
Iter    180 | loss 6.4302 | time 2576.1ms | lr 5.40e-04 | RMLP_w_std 0.897 | RMLP_w_rec 0.000
Iter    190 | loss 6.4257 | time 2576.1ms | lr 5.70e-04 | RMLP_w_std 0.897 | RMLP_w_rec 0.000
Iter    200 | train loss 6.3543 | val loss 6.2956 | lr 6.00e-04 | RMLP_w_std 0.897 | RMLP_w_rec 0.000
Saved best checkpoint (val_loss=6.2956)
Iter    200 | loss 6.4032 | time 2400.7ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.000
Iter    210 | loss 6.3963 | time 2576.7ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.000
Iter    220 | loss 6.2619 | time 2576.2ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.000
Iter    230 | loss 6.1953 | time 2576.2ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.000
Iter    240 | loss 6.1846 | time 2576.0ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.000
  Variance check at step 250: var=0.013636 (target<0.020000, min_step=250)

======================================================================
Step 250: Training stabilized (var=0.013636)
Unfreezing reciprocal gates (enabling RA/R-MLP)
======================================================================

Iter    250 | loss 6.2288 | time 2973.4ms | lr 6.00e-04 | RMLP_w_std 0.895 | RMLP_w_rec 0.000
Iter    260 | loss 6.1204 | time 2584.8ms | lr 6.00e-04 | RMLP_w_std 0.895 | RMLP_w_rec -0.000
Iter    270 | loss 6.1970 | time 2585.1ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec -0.001
Iter    280 | loss 6.1287 | time 2584.9ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.001
Iter    290 | loss 6.1102 | time 2585.7ms | lr 6.00e-04 | RMLP_w_std 0.896 | RMLP_w_rec 0.001
Iter    300 | train loss 6.0717 | val loss 6.0114 | lr 6.00e-04 | RMLP_w_std 0.897 | RMLP_w_rec 0.000
Saved best checkpoint (val_loss=6.0114)
Iter    300 | loss 6.1491 | time 2408.4ms | lr 6.00e-04 | RMLP_w_std 0.897 | RMLP_w_rec 0.000
Iter    310 | loss 6.0601 | time 2585.0ms | lr 6.00e-04 | RMLP_w_std 0.897 | RMLP_w_rec -0.001
Iter    320 | loss 6.0696 | time 2585.2ms | lr 6.00e-04 | RMLP_w_std 0.898 | RMLP_w_rec -0.001
Iter    330 | loss 5.9998 | time 2584.9ms | lr 6.00e-04 | RMLP_w_std 0.898 | RMLP_w_rec -0.000
Iter    340 | loss 5.9919 | time 2585.0ms | lr 6.00e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.000
Iter    350 | loss 5.8933 | time 2584.9ms | lr 6.00e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter    360 | loss 5.7121 | time 2585.8ms | lr 6.00e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.000
Iter    370 | loss 5.1324 | time 2584.8ms | lr 6.00e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.000
Iter    380 | loss 4.5507 | time 2584.7ms | lr 6.00e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.001
Iter    390 | loss 4.2782 | time 2585.3ms | lr 5.99e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.001
Iter    400 | train loss 4.0891 | val loss 4.0542 | lr 5.99e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.002
Saved best checkpoint (val_loss=4.0542)
Iter    400 | loss 4.1668 | time 2408.5ms | lr 5.99e-04 | RMLP_w_std 0.899 | RMLP_w_rec 0.002
Iter    410 | loss 4.0699 | time 2584.9ms | lr 5.99e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.003
Iter    420 | loss 3.9709 | time 2585.8ms | lr 5.99e-04 | RMLP_w_std 0.900 | RMLP_w_rec 0.004
Iter    430 | loss 3.9600 | time 2585.4ms | lr 5.99e-04 | RMLP_w_std 0.901 | RMLP_w_rec 0.005
Iter    440 | loss 3.9513 | time 2584.7ms | lr 5.99e-04 | RMLP_w_std 0.901 | RMLP_w_rec 0.007
Iter    450 | loss 3.8510 | time 2583.8ms | lr 5.99e-04 | RMLP_w_std 0.902 | RMLP_w_rec 0.008
Iter    460 | loss 3.8879 | time 2585.1ms | lr 5.99e-04 | RMLP_w_std 0.902 | RMLP_w_rec 0.009
Iter    470 | loss 3.8620 | time 2585.4ms | lr 5.99e-04 | RMLP_w_std 0.903 | RMLP_w_rec 0.011
Iter    480 | loss 3.7499 | time 2585.7ms | lr 5.99e-04 | RMLP_w_std 0.903 | RMLP_w_rec 0.012
Iter    490 | loss 3.7746 | time 2584.4ms | lr 5.99e-04 | RMLP_w_std 0.904 | RMLP_w_rec 0.014
Iter    500 | train loss 3.7135 | val loss 3.6888 | lr 5.99e-04 | RMLP_w_std 0.905 | RMLP_w_rec 0.015
Saved best checkpoint (val_loss=3.6888)
Iter    500 | loss 3.7424 | time 2407.7ms | lr 5.99e-04 | RMLP_w_std 0.905 | RMLP_w_rec 0.015
Iter    510 | loss 3.7332 | time 2584.4ms | lr 5.99e-04 | RMLP_w_std 0.905 | RMLP_w_rec 0.017
Iter    520 | loss 3.7069 | time 2584.5ms | lr 5.99e-04 | RMLP_w_std 0.906 | RMLP_w_rec 0.018
Iter    530 | loss 3.7251 | time 2584.6ms | lr 5.98e-04 | RMLP_w_std 0.907 | RMLP_w_rec 0.019
Iter    540 | loss 3.6917 | time 2584.9ms | lr 5.98e-04 | RMLP_w_std 0.907 | RMLP_w_rec 0.020
Iter    550 | loss 3.6636 | time 2584.3ms | lr 5.98e-04 | RMLP_w_std 0.908 | RMLP_w_rec 0.022
Iter    560 | loss 3.6964 | time 2584.1ms | lr 5.98e-04 | RMLP_w_std 0.909 | RMLP_w_rec 0.023
Iter    570 | loss 3.6960 | time 2584.6ms | lr 5.98e-04 | RMLP_w_std 0.910 | RMLP_w_rec 0.024
Iter    580 | loss 3.6651 | time 2585.0ms | lr 5.98e-04 | RMLP_w_std 0.910 | RMLP_w_rec 0.025
Iter    590 | loss 3.6750 | time 2584.6ms | lr 5.98e-04 | RMLP_w_std 0.911 | RMLP_w_rec 0.026
Iter    600 | train loss 3.6313 | val loss 3.6138 | lr 5.98e-04 | RMLP_w_std 0.912 | RMLP_w_rec 0.028
Saved best checkpoint (val_loss=3.6138)
Iter    600 | loss 3.6097 | time 2408.4ms | lr 5.98e-04 | RMLP_w_std 0.912 | RMLP_w_rec 0.028
Iter    610 | loss 3.6770 | time 2584.9ms | lr 5.98e-04 | RMLP_w_std 0.913 | RMLP_w_rec 0.029
Iter    620 | loss 3.6457 | time 2584.3ms | lr 5.98e-04 | RMLP_w_std 0.913 | RMLP_w_rec 0.030
Iter    630 | loss 3.6178 | time 2584.5ms | lr 5.97e-04 | RMLP_w_std 0.914 | RMLP_w_rec 0.030
Iter    640 | loss 3.6720 | time 2587.9ms | lr 5.97e-04 | RMLP_w_std 0.915 | RMLP_w_rec 0.032
Iter    650 | loss 3.6377 | time 2585.3ms | lr 5.97e-04 | RMLP_w_std 0.916 | RMLP_w_rec 0.033
Iter    660 | loss 3.6055 | time 2583.5ms | lr 5.97e-04 | RMLP_w_std 0.917 | RMLP_w_rec 0.034
Iter    670 | loss 3.6203 | time 2585.6ms | lr 5.97e-04 | RMLP_w_std 0.918 | RMLP_w_rec 0.034
Iter    680 | loss 3.5929 | time 2585.0ms | lr 5.97e-04 | RMLP_w_std 0.919 | RMLP_w_rec 0.035
Iter    690 | loss 3.6300 | time 2584.9ms | lr 5.97e-04 | RMLP_w_std 0.920 | RMLP_w_rec 0.037
Iter    700 | train loss 3.5738 | val loss 3.5736 | lr 5.97e-04 | RMLP_w_std 0.921 | RMLP_w_rec 0.038
Saved best checkpoint (val_loss=3.5736)
Iter    700 | loss 3.6193 | time 2408.7ms | lr 5.97e-04 | RMLP_w_std 0.921 | RMLP_w_rec 0.038
Iter    710 | loss 3.6013 | time 2584.7ms | lr 5.96e-04 | RMLP_w_std 0.922 | RMLP_w_rec 0.039
Iter    720 | loss 3.5831 | time 2584.5ms | lr 5.96e-04 | RMLP_w_std 0.923 | RMLP_w_rec 0.039
Iter    730 | loss 3.5973 | time 2584.6ms | lr 5.96e-04 | RMLP_w_std 0.924 | RMLP_w_rec 0.040
Iter    740 | loss 3.6150 | time 2584.8ms | lr 5.96e-04 | RMLP_w_std 0.925 | RMLP_w_rec 0.041
Iter    750 | loss 3.5699 | time 2584.8ms | lr 5.96e-04 | RMLP_w_std 0.926 | RMLP_w_rec 0.042
Iter    760 | loss 3.5805 | time 2584.3ms | lr 5.96e-04 | RMLP_w_std 0.927 | RMLP_w_rec 0.043
Iter    770 | loss 3.6088 | time 2584.4ms | lr 5.96e-04 | RMLP_w_std 0.928 | RMLP_w_rec 0.044
Iter    780 | loss 3.5847 | time 2585.1ms | lr 5.95e-04 | RMLP_w_std 0.929 | RMLP_w_rec 0.045
Iter    790 | loss 3.5490 | time 2584.7ms | lr 5.95e-04 | RMLP_w_std 0.929 | RMLP_w_rec 0.046
Iter    800 | train loss 3.5507 | val loss 3.5471 | lr 5.95e-04 | RMLP_w_std 0.931 | RMLP_w_rec 0.047
Saved best checkpoint (val_loss=3.5471)
Iter    800 | loss 3.5564 | time 2408.5ms | lr 5.95e-04 | RMLP_w_std 0.931 | RMLP_w_rec 0.047
Iter    810 | loss 3.6409 | time 2585.3ms | lr 5.95e-04 | RMLP_w_std 0.932 | RMLP_w_rec 0.048
Iter    820 | loss 3.5827 | time 2585.1ms | lr 5.95e-04 | RMLP_w_std 0.933 | RMLP_w_rec 0.049
Iter    830 | loss 3.5318 | time 2585.2ms | lr 5.95e-04 | RMLP_w_std 0.934 | RMLP_w_rec 0.049
Iter    840 | loss 3.5346 | time 2584.6ms | lr 5.94e-04 | RMLP_w_std 0.935 | RMLP_w_rec 0.050
Iter    850 | loss 3.5447 | time 2584.0ms | lr 5.94e-04 | RMLP_w_std 0.936 | RMLP_w_rec 0.051
Iter    860 | loss 3.5089 | time 2584.5ms | lr 5.94e-04 | RMLP_w_std 0.938 | RMLP_w_rec 0.052
Iter    870 | loss 3.5603 | time 2585.3ms | lr 5.94e-04 | RMLP_w_std 0.939 | RMLP_w_rec 0.053
Iter    880 | loss 3.5371 | time 2584.7ms | lr 5.94e-04 | RMLP_w_std 0.940 | RMLP_w_rec 0.054
Iter    890 | loss 3.5736 | time 2584.6ms | lr 5.93e-04 | RMLP_w_std 0.941 | RMLP_w_rec 0.055
Iter    900 | train loss 3.5302 | val loss 3.5090 | lr 5.93e-04 | RMLP_w_std 0.942 | RMLP_w_rec 0.055
Saved best checkpoint (val_loss=3.5090)
Iter    900 | loss 3.5264 | time 2409.6ms | lr 5.93e-04 | RMLP_w_std 0.942 | RMLP_w_rec 0.055
Iter    910 | loss 3.5754 | time 2584.5ms | lr 5.93e-04 | RMLP_w_std 0.943 | RMLP_w_rec 0.056
Iter    920 | loss 3.5766 | time 2585.0ms | lr 5.93e-04 | RMLP_w_std 0.944 | RMLP_w_rec 0.057
Iter    930 | loss 3.5476 | time 2585.2ms | lr 5.93e-04 | RMLP_w_std 0.946 | RMLP_w_rec 0.058
Iter    940 | loss 3.5240 | time 2584.8ms | lr 5.92e-04 | RMLP_w_std 0.947 | RMLP_w_rec 0.059
Iter    950 | loss 3.5423 | time 2585.5ms | lr 5.92e-04 | RMLP_w_std 0.948 | RMLP_w_rec 0.060
Iter    960 | loss 3.5295 | time 2584.2ms | lr 5.92e-04 | RMLP_w_std 0.949 | RMLP_w_rec 0.061
Iter    970 | loss 3.5369 | time 2585.0ms | lr 5.92e-04 | RMLP_w_std 0.951 | RMLP_w_rec 0.062
Iter    980 | loss 3.5223 | time 2584.9ms | lr 5.92e-04 | RMLP_w_std 0.952 | RMLP_w_rec 0.062
Iter    990 | loss 3.4731 | time 2584.5ms | lr 5.91e-04 | RMLP_w_std 0.953 | RMLP_w_rec 0.063
Iter   1000 | train loss 3.5035 | val loss 3.5027 | lr 5.91e-04 | RMLP_w_std 0.954 | RMLP_w_rec 0.064
Saved best checkpoint (val_loss=3.5027)
Iter   1000 | loss 3.4845 | time 2409.4ms | lr 5.91e-04 | RMLP_w_std 0.954 | RMLP_w_rec 0.064
Iter   1010 | loss 3.5271 | time 2584.5ms | lr 5.91e-04 | RMLP_w_std 0.955 | RMLP_w_rec 0.064
Iter   1020 | loss 3.4853 | time 2584.0ms | lr 5.91e-04 | RMLP_w_std 0.956 | RMLP_w_rec 0.065
Iter   1030 | loss 3.5049 | time 2585.0ms | lr 5.90e-04 | RMLP_w_std 0.958 | RMLP_w_rec 0.067
Iter   1040 | loss 3.5432 | time 2584.6ms | lr 5.90e-04 | RMLP_w_std 0.959 | RMLP_w_rec 0.068
Iter   1050 | loss 3.5590 | time 2584.6ms | lr 5.90e-04 | RMLP_w_std 0.960 | RMLP_w_rec 0.068
Iter   1060 | loss 3.5443 | time 2584.5ms | lr 5.90e-04 | RMLP_w_std 0.961 | RMLP_w_rec 0.069
Iter   1070 | loss 3.4682 | time 2585.5ms | lr 5.90e-04 | RMLP_w_std 0.962 | RMLP_w_rec 0.070
Iter   1080 | loss 3.4767 | time 2584.9ms | lr 5.89e-04 | RMLP_w_std 0.964 | RMLP_w_rec 0.070
Iter   1090 | loss 3.5271 | time 2584.3ms | lr 5.89e-04 | RMLP_w_std 0.965 | RMLP_w_rec 0.071
Iter   1100 | train loss 3.4741 | val loss 3.4928 | lr 5.89e-04 | RMLP_w_std 0.966 | RMLP_w_rec 0.071
Saved best checkpoint (val_loss=3.4928)
Iter   1100 | loss 3.4012 | time 2409.0ms | lr 5.89e-04 | RMLP_w_std 0.966 | RMLP_w_rec 0.071
Iter   1110 | loss 3.5127 | time 2584.7ms | lr 5.89e-04 | RMLP_w_std 0.967 | RMLP_w_rec 0.072
Iter   1120 | loss 3.4841 | time 2584.5ms | lr 5.88e-04 | RMLP_w_std 0.969 | RMLP_w_rec 0.073
Iter   1130 | loss 3.5320 | time 2584.0ms | lr 5.88e-04 | RMLP_w_std 0.970 | RMLP_w_rec 0.074
Iter   1140 | loss 3.5157 | time 2585.3ms | lr 5.88e-04 | RMLP_w_std 0.971 | RMLP_w_rec 0.075
Iter   1150 | loss 3.4730 | time 2584.4ms | lr 5.88e-04 | RMLP_w_std 0.972 | RMLP_w_rec 0.076
Iter   1160 | loss 3.5200 | time 2584.7ms | lr 5.87e-04 | RMLP_w_std 0.973 | RMLP_w_rec 0.077
Iter   1170 | loss 3.4794 | time 2584.4ms | lr 5.87e-04 | RMLP_w_std 0.974 | RMLP_w_rec 0.077
Iter   1180 | loss 3.4922 | time 2585.2ms | lr 5.87e-04 | RMLP_w_std 0.975 | RMLP_w_rec 0.078
Iter   1190 | loss 3.4712 | time 2584.4ms | lr 5.87e-04 | RMLP_w_std 0.976 | RMLP_w_rec 0.079
Iter   1200 | train loss 3.4662 | val loss 3.4674 | lr 5.86e-04 | RMLP_w_std 0.977 | RMLP_w_rec 0.079
Saved best checkpoint (val_loss=3.4674)
Iter   1200 | loss 3.4634 | time 2409.0ms | lr 5.86e-04 | RMLP_w_std 0.978 | RMLP_w_rec 0.080
Iter   1210 | loss 3.4785 | time 2584.8ms | lr 5.86e-04 | RMLP_w_std 0.979 | RMLP_w_rec 0.080
Iter   1220 | loss 3.4870 | time 2584.9ms | lr 5.86e-04 | RMLP_w_std 0.980 | RMLP_w_rec 0.081
Iter   1230 | loss 3.5031 | time 2585.1ms | lr 5.85e-04 | RMLP_w_std 0.981 | RMLP_w_rec 0.082
Iter   1240 | loss 3.5044 | time 2585.0ms | lr 5.85e-04 | RMLP_w_std 0.983 | RMLP_w_rec 0.083
Iter   1250 | loss 3.4894 | time 2584.5ms | lr 5.85e-04 | RMLP_w_std 0.984 | RMLP_w_rec 0.084
Iter   1260 | loss 3.5029 | time 2585.4ms | lr 5.85e-04 | RMLP_w_std 0.985 | RMLP_w_rec 0.084
Iter   1270 | loss 3.5003 | time 2584.3ms | lr 5.84e-04 | RMLP_w_std 0.986 | RMLP_w_rec 0.085
Iter   1280 | loss 3.4452 | time 2584.2ms | lr 5.84e-04 | RMLP_w_std 0.987 | RMLP_w_rec 0.086
Iter   1290 | loss 3.4890 | time 2584.7ms | lr 5.84e-04 | RMLP_w_std 0.988 | RMLP_w_rec 0.087
Iter   1300 | train loss 3.4513 | val loss 3.4512 | lr 5.83e-04 | RMLP_w_std 0.989 | RMLP_w_rec 0.087
Saved best checkpoint (val_loss=3.4512)
Iter   1300 | loss 3.4866 | time 2408.8ms | lr 5.83e-04 | RMLP_w_std 0.989 | RMLP_w_rec 0.087
Iter   1310 | loss 3.4761 | time 2584.8ms | lr 5.83e-04 | RMLP_w_std 0.990 | RMLP_w_rec 0.088
Iter   1320 | loss 3.4662 | time 2584.6ms | lr 5.83e-04 | RMLP_w_std 0.991 | RMLP_w_rec 0.089
Iter   1330 | loss 3.4251 | time 2584.7ms | lr 5.82e-04 | RMLP_w_std 0.993 | RMLP_w_rec 0.089
Iter   1340 | loss 3.4718 | time 2583.7ms | lr 5.82e-04 | RMLP_w_std 0.994 | RMLP_w_rec 0.090
Iter   1350 | loss 3.4173 | time 2584.9ms | lr 5.82e-04 | RMLP_w_std 0.995 | RMLP_w_rec 0.091
Iter   1360 | loss 3.4333 | time 2584.5ms | lr 5.82e-04 | RMLP_w_std 0.996 | RMLP_w_rec 0.092
Iter   1370 | loss 3.4939 | time 2585.1ms | lr 5.81e-04 | RMLP_w_std 0.997 | RMLP_w_rec 0.092
Iter   1380 | loss 3.4082 | time 2585.4ms | lr 5.81e-04 | RMLP_w_std 0.998 | RMLP_w_rec 0.093
Iter   1390 | loss 3.4805 | time 2584.4ms | lr 5.81e-04 | RMLP_w_std 0.999 | RMLP_w_rec 0.094
Iter   1400 | train loss 3.4192 | val loss 3.4505 | lr 5.80e-04 | RMLP_w_std 1.000 | RMLP_w_rec 0.094
Saved best checkpoint (val_loss=3.4505)
Iter   1400 | loss 3.4949 | time 2408.3ms | lr 5.80e-04 | RMLP_w_std 1.000 | RMLP_w_rec 0.095
Iter   1410 | loss 3.4122 | time 2584.5ms | lr 5.80e-04 | RMLP_w_std 1.001 | RMLP_w_rec 0.095
Iter   1420 | loss 3.4426 | time 2584.7ms | lr 5.80e-04 | RMLP_w_std 1.002 | RMLP_w_rec 0.095
Iter   1430 | loss 3.4641 | time 2585.2ms | lr 5.79e-04 | RMLP_w_std 1.002 | RMLP_w_rec 0.096
Iter   1440 | loss 3.4604 | time 2585.3ms | lr 5.79e-04 | RMLP_w_std 1.003 | RMLP_w_rec 0.096
Iter   1450 | loss 3.4861 | time 2584.8ms | lr 5.79e-04 | RMLP_w_std 1.004 | RMLP_w_rec 0.097
Iter   1460 | loss 3.4790 | time 2581.3ms | lr 5.78e-04 | RMLP_w_std 1.005 | RMLP_w_rec 0.098
Iter   1470 | loss 3.4204 | time 2584.5ms | lr 5.78e-04 | RMLP_w_std 1.006 | RMLP_w_rec 0.098
Iter   1480 | loss 3.4766 | time 2585.3ms | lr 5.78e-04 | RMLP_w_std 1.006 | RMLP_w_rec 0.099
Iter   1490 | loss 3.4408 | time 2584.6ms | lr 5.77e-04 | RMLP_w_std 1.007 | RMLP_w_rec 0.100
Iter   1500 | train loss 3.4027 | val loss 3.4287 | lr 5.77e-04 | RMLP_w_std 1.008 | RMLP_w_rec 0.100
Saved best checkpoint (val_loss=3.4287)
Iter   1500 | loss 3.4284 | time 2408.5ms | lr 5.77e-04 | RMLP_w_std 1.008 | RMLP_w_rec 0.100
Iter   1510 | loss 3.4264 | time 2585.5ms | lr 5.77e-04 | RMLP_w_std 1.008 | RMLP_w_rec 0.101
Iter   1520 | loss 3.4246 | time 2585.8ms | lr 5.76e-04 | RMLP_w_std 1.009 | RMLP_w_rec 0.102
Iter   1530 | loss 3.4189 | time 2585.4ms | lr 5.76e-04 | RMLP_w_std 1.010 | RMLP_w_rec 0.102
Iter   1540 | loss 3.4159 | time 2585.0ms | lr 5.75e-04 | RMLP_w_std 1.010 | RMLP_w_rec 0.102
Iter   1550 | loss 3.3868 | time 2584.8ms | lr 5.75e-04 | RMLP_w_std 1.010 | RMLP_w_rec 0.103
Iter   1560 | loss 3.4078 | time 2584.9ms | lr 5.75e-04 | RMLP_w_std 1.010 | RMLP_w_rec 0.103
Iter   1570 | loss 3.4316 | time 2585.4ms | lr 5.74e-04 | RMLP_w_std 1.011 | RMLP_w_rec 0.104
Iter   1580 | loss 3.4162 | time 2584.4ms | lr 5.74e-04 | RMLP_w_std 1.011 | RMLP_w_rec 0.104
Iter   1590 | loss 3.4332 | time 2585.1ms | lr 5.74e-04 | RMLP_w_std 1.012 | RMLP_w_rec 0.104
Iter   1600 | train loss 3.3839 | val loss 3.3946 | lr 5.73e-04 | RMLP_w_std 1.012 | RMLP_w_rec 0.105
Saved best checkpoint (val_loss=3.3946)
Iter   1600 | loss 3.4027 | time 2408.5ms | lr 5.73e-04 | RMLP_w_std 1.012 | RMLP_w_rec 0.105
Iter   1610 | loss 3.4301 | time 2584.5ms | lr 5.73e-04 | RMLP_w_std 1.012 | RMLP_w_rec 0.106
Iter   1620 | loss 3.4412 | time 2584.5ms | lr 5.73e-04 | RMLP_w_std 1.013 | RMLP_w_rec 0.106
Iter   1630 | loss 3.3481 | time 2584.4ms | lr 5.72e-04 | RMLP_w_std 1.013 | RMLP_w_rec 0.106
Iter   1640 | loss 3.3930 | time 2584.3ms | lr 5.72e-04 | RMLP_w_std 1.014 | RMLP_w_rec 0.107
Iter   1650 | loss 3.4517 | time 2584.6ms | lr 5.71e-04 | RMLP_w_std 1.014 | RMLP_w_rec 0.108
Iter   1660 | loss 3.4106 | time 2584.6ms | lr 5.71e-04 | RMLP_w_std 1.014 | RMLP_w_rec 0.108
Iter   1670 | loss 3.4296 | time 2584.5ms | lr 5.71e-04 | RMLP_w_std 1.015 | RMLP_w_rec 0.109
Iter   1680 | loss 3.3655 | time 2584.2ms | lr 5.70e-04 | RMLP_w_std 1.015 | RMLP_w_rec 0.109
Iter   1690 | loss 3.4612 | time 2586.1ms | lr 5.70e-04 | RMLP_w_std 1.015 | RMLP_w_rec 0.110
Iter   1700 | train loss 3.3638 | val loss 3.3775 | lr 5.69e-04 | RMLP_w_std 1.016 | RMLP_w_rec 0.110
Saved best checkpoint (val_loss=3.3775)
Iter   1700 | loss 3.4000 | time 2409.2ms | lr 5.69e-04 | RMLP_w_std 1.016 | RMLP_w_rec 0.111
Iter   1710 | loss 3.4190 | time 2585.1ms | lr 5.69e-04 | RMLP_w_std 1.016 | RMLP_w_rec 0.111
Iter   1720 | loss 3.3817 | time 2586.3ms | lr 5.69e-04 | RMLP_w_std 1.017 | RMLP_w_rec 0.112
Iter   1730 | loss 3.4129 | time 2584.1ms | lr 5.68e-04 | RMLP_w_std 1.017 | RMLP_w_rec 0.112
Iter   1740 | loss 3.3897 | time 2584.9ms | lr 5.68e-04 | RMLP_w_std 1.017 | RMLP_w_rec 0.113
Iter   1750 | loss 3.3803 | time 2584.2ms | lr 5.67e-04 | RMLP_w_std 1.017 | RMLP_w_rec 0.113
Iter   1760 | loss 3.3215 | time 2585.8ms | lr 5.67e-04 | RMLP_w_std 1.018 | RMLP_w_rec 0.114
Iter   1770 | loss 3.3631 | time 2584.5ms | lr 5.67e-04 | RMLP_w_std 1.019 | RMLP_w_rec 0.114
Iter   1780 | loss 3.3376 | time 2584.4ms | lr 5.66e-04 | RMLP_w_std 1.020 | RMLP_w_rec 0.115
Iter   1790 | loss 3.3800 | time 2585.9ms | lr 5.66e-04 | RMLP_w_std 1.021 | RMLP_w_rec 0.116
Iter   1800 | train loss 3.3397 | val loss 3.3438 | lr 5.65e-04 | RMLP_w_std 1.022 | RMLP_w_rec 0.117
Saved best checkpoint (val_loss=3.3438)
Iter   1800 | loss 3.3662 | time 2409.6ms | lr 5.65e-04 | RMLP_w_std 1.022 | RMLP_w_rec 0.117
Iter   1810 | loss 3.3941 | time 2584.5ms | lr 5.65e-04 | RMLP_w_std 1.022 | RMLP_w_rec 0.118
Iter   1820 | loss 3.3651 | time 2583.9ms | lr 5.64e-04 | RMLP_w_std 1.023 | RMLP_w_rec 0.118
Iter   1830 | loss 3.3773 | time 2584.9ms | lr 5.64e-04 | RMLP_w_std 1.024 | RMLP_w_rec 0.119
Iter   1840 | loss 3.3621 | time 2584.8ms | lr 5.64e-04 | RMLP_w_std 1.024 | RMLP_w_rec 0.119
Iter   1850 | loss 3.3586 | time 2584.2ms | lr 5.63e-04 | RMLP_w_std 1.025 | RMLP_w_rec 0.119
Iter   1860 | loss 3.3257 | time 2584.7ms | lr 5.63e-04 | RMLP_w_std 1.026 | RMLP_w_rec 0.120
Iter   1870 | loss 3.4067 | time 2584.3ms | lr 5.62e-04 | RMLP_w_std 1.026 | RMLP_w_rec 0.121
Iter   1880 | loss 3.3404 | time 2584.2ms | lr 5.62e-04 | RMLP_w_std 1.027 | RMLP_w_rec 0.121
Iter   1890 | loss 3.3704 | time 2584.7ms | lr 5.61e-04 | RMLP_w_std 1.028 | RMLP_w_rec 0.122
Iter   1900 | train loss 3.3278 | val loss 3.3321 | lr 5.61e-04 | RMLP_w_std 1.029 | RMLP_w_rec 0.122
Saved best checkpoint (val_loss=3.3321)
Iter   1900 | loss 3.3126 | time 2409.3ms | lr 5.61e-04 | RMLP_w_std 1.029 | RMLP_w_rec 0.122
Iter   1910 | loss 3.3470 | time 2584.1ms | lr 5.60e-04 | RMLP_w_std 1.029 | RMLP_w_rec 0.123
Iter   1920 | loss 3.3747 | time 2584.8ms | lr 5.60e-04 | RMLP_w_std 1.030 | RMLP_w_rec 0.124
Iter   1930 | loss 3.3588 | time 2585.5ms | lr 5.60e-04 | RMLP_w_std 1.031 | RMLP_w_rec 0.124
Iter   1940 | loss 3.2830 | time 2584.4ms | lr 5.59e-04 | RMLP_w_std 1.032 | RMLP_w_rec 0.125
Iter   1950 | loss 3.3447 | time 2584.6ms | lr 5.59e-04 | RMLP_w_std 1.032 | RMLP_w_rec 0.125
Iter   1960 | loss 3.3874 | time 2585.0ms | lr 5.58e-04 | RMLP_w_std 1.033 | RMLP_w_rec 0.126
Iter   1970 | loss 3.3457 | time 2584.9ms | lr 5.58e-04 | RMLP_w_std 1.034 | RMLP_w_rec 0.127
Iter   1980 | loss 3.3069 | time 2585.6ms | lr 5.57e-04 | RMLP_w_std 1.035 | RMLP_w_rec 0.127
Iter   1990 | loss 3.3561 | time 2585.3ms | lr 5.57e-04 | RMLP_w_std 1.035 | RMLP_w_rec 0.128
Iter   2000 | train loss 3.3067 | val loss 3.3222 | lr 5.56e-04 | RMLP_w_std 1.036 | RMLP_w_rec 0.128
Saved best checkpoint (val_loss=3.3222)
Iter   2000 | loss 3.3576 | time 2409.4ms | lr 5.56e-04 | RMLP_w_std 1.036 | RMLP_w_rec 0.128
Iter   2010 | loss 3.3320 | time 2584.9ms | lr 5.56e-04 | RMLP_w_std 1.037 | RMLP_w_rec 0.129
Iter   2020 | loss 3.3503 | time 2584.7ms | lr 5.55e-04 | RMLP_w_std 1.037 | RMLP_w_rec 0.129
Iter   2030 | loss 3.3491 | time 2584.8ms | lr 5.55e-04 | RMLP_w_std 1.038 | RMLP_w_rec 0.130
Iter   2040 | loss 3.3284 | time 2584.8ms | lr 5.54e-04 | RMLP_w_std 1.039 | RMLP_w_rec 0.130
Iter   2050 | loss 3.2769 | time 2584.7ms | lr 5.54e-04 | RMLP_w_std 1.040 | RMLP_w_rec 0.131
Iter   2060 | loss 3.3505 | time 2584.4ms | lr 5.53e-04 | RMLP_w_std 1.040 | RMLP_w_rec 0.132

Reached time limit: 2.00 hours
Completed 2065 iterations

Saved final checkpoint to checkpoints_ra_mla/final_model.pt
Saved metrics to /opt/dlami/nvme/AdamWPrune/test_matrix_results_20251111_170325/gpt2_adamwspam_ramla_stepV18/training_metrics.json

Final evaluation:
Train loss: 3.3045
Val loss: 3.3106
Best val loss: 3.3222
  Avg iteration time: 2575.9ms
* Run finished. Uploading logs to Trackio (please wait...)
wandb: updating run metadata
wandb: uploading history steps 228-228, summary, console lines 351-355
wandb: 
wandb: Run history:
wandb: avg_forward_time_ms â–
wandb:       best_val_loss  â–ˆâ–„â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    final_train_loss â–
wandb:      final_val_loss â–
wandb:     forward_time_ms â–ƒâ–ˆâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒ
wandb:           iteration â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:       learning_rate â–â–‚â–ƒâ–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡
wandb:      rmlp_w_rec_max â–â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     rmlp_w_rec_mean â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:      rmlp_w_rec_min â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                 +10 ...
wandb: 
wandb: Run summary:
wandb: avg_forward_time_ms 2575.90376
wandb:       best_val_loss 3.32221
wandb:    final_train_loss 3.30451
wandb:      final_val_loss 3.31061
wandb:     forward_time_ms 2584.4326
wandb:           iteration 2060
wandb:       learning_rate 0.00055
wandb:      rmlp_w_rec_max 0.57387
wandb:     rmlp_w_rec_mean 0.13179
wandb:      rmlp_w_rec_min -0.57306
wandb:                 +10 ...
wandb: 
wandb: ğŸš€ View run gpt2_adamwspam_ramla_stepV18 at: https://wandb.ai/mcgrof-citizen/gpt2-kv-pruning/runs/8aeiii5q
wandb: â­ï¸ View project at: https://wandb.ai/mcgrof-citizen/gpt2-kv-pruning
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20251111_231006-8aeiii5q/logs
2025-11-12 01:11:42,590 - INFO - Training completed successfully
2025-11-12 01:11:43,172 - INFO - Simple GPU monitoring completed
2025-11-12 01:11:43,633 - WARNING - Graph generation failed: 
2025-11-12 01:11:43,633 - INFO - GPU stats saved to: test_matrix_results_20251111_170325/gpt2_adamwspam_ramla_stepV18/gpu_stats_gpt2_adamwspam_ramla_stepV18_20251111_230951.json
2025-11-12 01:11:43,633 - INFO - Generating performance graphs...
2025-11-12 01:11:44,094 - ERROR - Failed to generate graphs: 
2025-11-12 01:11:44,094 - WARNING - Failed to generate graphs
2025-11-12 01:11:44,094 - INFO - Training with monitoring completed successfully!
