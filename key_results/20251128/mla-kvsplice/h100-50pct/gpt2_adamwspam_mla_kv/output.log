/mnt/tmpfs/knlp/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-28 11:27:52,755 - INFO - Training configuration: gpt2_adamwspam_mla_kv
2025-11-28 11:27:52,756 - INFO - DDP not enabled in config - using single-GPU mode
2025-11-28 11:27:52,756 - INFO - Starting training with GPU monitoring: gpt2_adamwspam_mla_kv
2025-11-28 11:27:52,773 - INFO - Started simple GPU monitoring - stats will be saved to test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/gpu_stats_gpt2_adamwspam_mla_kv_20251128_112752.json
2025-11-28 11:27:52,773 - INFO - Command: /mnt/tmpfs/knlp/.venv/bin/python3 /mnt/tmpfs/knlp/gpt2/train.py --output-dir test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv --optimizer adamwspam --dataset tinystories --data-dir ./gpt2/data --block-size 1024 --flash-attention --warmup-steps 2000 --eval-interval 50 --eval-samples 200 --max-iters 10000 --max-time 7200 --decay-lr --min-lr 6e-5 --learning-rate 6e-4 --pruning-method none --spam-theta 50.0 --spam-interval 1000 --spam-warmup-steps 1000 --spam-enable-clip --json-output /mnt/tmpfs/knlp/test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/training_metrics.json --tracker wandb,trackio --tracker-project gpt2-kvsplice-ablation-h100 --tracker-run-name gpt2_adamwspam_mla_kv
/mnt/tmpfs/knlp/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Hyperparams: AUTO mode - GPU: NVIDIA H100 80GB HBM3 (85.0GB total, 84.5GB free), model=gpt2+MLA, compile=ON ‚Üí batch=76, grad_acc=3 (per_gpu_eff=256, total_eff=228, target=256)
Compile: Enabled (GPU 'NVIDIA H100 80GB HBM3' has good torch.compile support)
Running Vanilla GPT-2 trainer
* Trackio project initialized: gpt2-kvsplice-ablation-h100
* Trackio metrics logged to: /root/.cache/huggingface/trackio
* View dashboard by running in your terminal:
[1m[38;5;208mtrackio show --project "gpt2-kvsplice-ablation-h100"[0m
* or by running in Python: trackio.show(project="gpt2-kvsplice-ablation-h100")
* Created new run: gpt2_adamwspam_mla_kv
Initialized Trackio tracking for project: gpt2-kvsplice-ablation-h100
wandb: Currently logged in as: mcgrof (mcgrof-citizen) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run vzaax44e
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /mnt/tmpfs/knlp/wandb/run-20251128_112758-vzaax44e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gpt2_adamwspam_mla_kv
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-h100
wandb: üöÄ View run at https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-h100/runs/vzaax44e
wandb: Initializing weave.
[36m[1mweave[0m: Logged in as Weights & Biases user: mcgrof.
[36m[1mweave[0m: View Weave data at https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-h100/weave
Initialized WandB tracking for project: gpt2-kvsplice-ablation-h100
Initializing MLA model: mla_kv
  MLA+KVSplice: d_latent=256, compression_ratio=0.5
Setting up adamwspam optimizer...
Weight decay: 0.1

Starting training...
Parameters: 118.16M
Device: cuda, dtype: bfloat16
Batch size: 76, Gradient accumulation: 3
Effective batch size: 228
Save checkpoint: True, Output: test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv
--------------------------------------------------
Iter     0 | loss 1.0965 | ppl    2.99 | lr 0.00e+00 | sparsity 0.0% | 306.8ms/iter

Eval @ iter 0: train 10.9590, val 10.9600, ppl 57525.18

--- lm-eval Benchmarks ---
Running lm-eval with limit=100 samples per task
Overwriting default num_fewshot of hellaswag from None to 0

  0%|          | 0/100 [00:00<?, ?it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 4307.99it/s]
hellaswag/acc,none: 0.2700
hellaswag/acc_stderr,none: 0.0446
hellaswag/acc_norm,none: 0.3000
hellaswag/acc_norm_stderr,none: 0.0461
Created lm-eval results table with 1 tasks

Generated 3 text samples
Saved checkpoint at iteration 0
Iter    10 | loss 10.8931 | ppl 53804.73 | lr 3.00e-06 | sparsity 0.0% | 19218.2ms/iter
Iter    20 | loss 10.4369 | ppl 34093.48 | lr 6.00e-06 | sparsity 0.0% | 2912.0ms/iter
Iter    30 | loss 9.6616 | ppl 15702.42 | lr 9.00e-06 | sparsity 0.0% | 2894.6ms/iter
Iter    40 | loss 8.9492 | ppl 7702.08 | lr 1.20e-05 | sparsity 0.0% | 2887.5ms/iter
Iter    50 | loss 8.3441 | ppl 4205.20 | lr 1.50e-05 | sparsity 0.0% | 2880.5ms/iter

Eval @ iter 50: train 7.9515, val 7.9442, ppl 2819.13

--- KV Cache Memory ---
  Cache type: kvsplice
  seq=512: 1.5 MB (92% savings)
  seq=1024: 3.0 MB (92% savings)
  seq=2048: 6.0 MB (92% savings)
  seq=4096: 12.0 MB (92% savings)

Generated 3 text samples
Iter    60 | loss 7.7250 | ppl 2264.36 | lr 1.80e-05 | sparsity 0.0% | 17671.4ms/iter
Iter    70 | loss 7.1123 | ppl 1227.02 | lr 2.10e-05 | sparsity 0.0% | 2871.3ms/iter
Iter    80 | loss 6.5208 | ppl  679.09 | lr 2.40e-05 | sparsity 0.0% | 2866.3ms/iter
Iter    90 | loss 5.9663 | ppl  390.08 | lr 2.70e-05 | sparsity 0.0% | 2865.8ms/iter
Iter   100 | loss 5.5286 | ppl  251.78 | lr 3.00e-05 | sparsity 0.0% | 2865.3ms/iter

Eval @ iter 100: train 5.2564, val 5.2563, ppl 191.78

Generated 3 text samples
Iter   110 | loss 5.1766 | ppl  177.08 | lr 3.30e-05 | sparsity 0.0% | 17662.4ms/iter
Iter   120 | loss 4.9178 | ppl  136.69 | lr 3.60e-05 | sparsity 0.0% | 2862.9ms/iter
Iter   130 | loss 4.7091 | ppl  110.95 | lr 3.90e-05 | sparsity 0.0% | 2861.6ms/iter
Iter   140 | loss 4.5402 | ppl   93.71 | lr 4.20e-05 | sparsity 0.0% | 2867.4ms/iter
Iter   150 | loss 4.4148 | ppl   82.66 | lr 4.50e-05 | sparsity 0.0% | 2860.7ms/iter

Eval @ iter 150: train 4.3010, val 4.3039, ppl 73.99

Generated 3 text samples
Iter   160 | loss 4.3041 | ppl   74.00 | lr 4.80e-05 | sparsity 0.0% | 17678.9ms/iter
Iter   170 | loss 4.2202 | ppl   68.05 | lr 5.10e-05 | sparsity 0.0% | 2864.5ms/iter
Iter   180 | loss 4.1393 | ppl   62.76 | lr 5.40e-05 | sparsity 0.0% | 2864.8ms/iter
Iter   190 | loss 4.0693 | ppl   58.51 | lr 5.70e-05 | sparsity 0.0% | 2870.3ms/iter
Iter   200 | loss 4.0403 | ppl   56.84 | lr 6.00e-05 | sparsity 0.0% | 2876.7ms/iter

Eval @ iter 200: train 3.9779, val 3.9771, ppl 53.36

Generated 3 text samples
Iter   210 | loss 4.0125 | ppl   55.29 | lr 6.30e-05 | sparsity 0.0% | 17678.7ms/iter
Iter   220 | loss 3.9892 | ppl   54.01 | lr 6.60e-05 | sparsity 0.0% | 2884.4ms/iter
Iter   230 | loss 3.9356 | ppl   51.20 | lr 6.90e-05 | sparsity 0.0% | 2879.1ms/iter
Iter   240 | loss 3.8675 | ppl   47.82 | lr 7.20e-05 | sparsity 0.0% | 2872.7ms/iter
Iter   250 | loss 3.8157 | ppl   45.41 | lr 7.50e-05 | sparsity 0.0% | 2874.8ms/iter

Eval @ iter 250: train 3.7486, val 3.7447, ppl 42.30

Generated 3 text samples
Iter   260 | loss 3.7680 | ppl   43.30 | lr 7.80e-05 | sparsity 0.0% | 17706.3ms/iter
Iter   270 | loss 3.7100 | ppl   40.85 | lr 8.10e-05 | sparsity 0.0% | 2873.0ms/iter
Iter   280 | loss 3.6583 | ppl   38.80 | lr 8.40e-05 | sparsity 0.0% | 2877.6ms/iter
Iter   290 | loss 3.6200 | ppl   37.34 | lr 8.70e-05 | sparsity 0.0% | 2879.0ms/iter
Iter   300 | loss 3.5853 | ppl   36.07 | lr 9.00e-05 | sparsity 0.0% | 2878.2ms/iter

Eval @ iter 300: train 3.5100, val 3.5099, ppl 33.44

Generated 3 text samples
Iter   310 | loss 3.5318 | ppl   34.19 | lr 9.30e-05 | sparsity 0.0% | 17695.8ms/iter
Iter   320 | loss 3.4889 | ppl   32.75 | lr 9.60e-05 | sparsity 0.0% | 2878.0ms/iter
Iter   330 | loss 3.4370 | ppl   31.09 | lr 9.90e-05 | sparsity 0.0% | 2879.8ms/iter
Iter   340 | loss 3.3898 | ppl   29.66 | lr 1.02e-04 | sparsity 0.0% | 2877.9ms/iter
Iter   350 | loss 3.3518 | ppl   28.55 | lr 1.05e-04 | sparsity 0.0% | 2879.1ms/iter

Eval @ iter 350: train 3.2657, val 3.2674, ppl 26.24

Generated 3 text samples
Iter   360 | loss 3.3084 | ppl   27.34 | lr 1.08e-04 | sparsity 0.0% | 17676.1ms/iter
Iter   370 | loss 3.2564 | ppl   25.96 | lr 1.11e-04 | sparsity 0.0% | 2885.4ms/iter
Iter   380 | loss 3.2171 | ppl   24.95 | lr 1.14e-04 | sparsity 0.0% | 2881.3ms/iter
Iter   390 | loss 3.1814 | ppl   24.08 | lr 1.17e-04 | sparsity 0.0% | 2882.6ms/iter
Iter   400 | loss 3.1274 | ppl   22.81 | lr 1.20e-04 | sparsity 0.0% | 2875.0ms/iter

Eval @ iter 400: train 3.0600, val 3.0579, ppl 21.28

Generated 3 text samples
Iter   410 | loss 3.0924 | ppl   22.03 | lr 1.23e-04 | sparsity 0.0% | 17686.0ms/iter
Iter   420 | loss 3.0481 | ppl   21.08 | lr 1.26e-04 | sparsity 0.0% | 2872.6ms/iter
Iter   430 | loss 3.0040 | ppl   20.17 | lr 1.29e-04 | sparsity 0.0% | 2869.5ms/iter
Iter   440 | loss 2.9736 | ppl   19.56 | lr 1.32e-04 | sparsity 0.0% | 2871.4ms/iter
Iter   450 | loss 2.9217 | ppl   18.57 | lr 1.35e-04 | sparsity 0.0% | 2875.7ms/iter

Eval @ iter 450: train 2.8393, val 2.8417, ppl 17.14

Generated 3 text samples
Iter   460 | loss 2.8875 | ppl   17.95 | lr 1.38e-04 | sparsity 0.0% | 17678.4ms/iter
Iter   470 | loss 2.8388 | ppl   17.10 | lr 1.41e-04 | sparsity 0.0% | 2870.1ms/iter
Iter   480 | loss 2.7971 | ppl   16.40 | lr 1.44e-04 | sparsity 0.0% | 2863.7ms/iter
Iter   490 | loss 2.7726 | ppl   16.00 | lr 1.47e-04 | sparsity 0.0% | 2868.9ms/iter
Iter   500 | loss 2.7362 | ppl   15.43 | lr 1.50e-04 | sparsity 0.0% | 2869.9ms/iter

Eval @ iter 500: train 2.6552, val 2.6539, ppl 14.21

Generated 3 text samples
Iter   510 | loss 2.7259 | ppl   15.27 | lr 1.53e-04 | sparsity 0.0% | 17711.4ms/iter
Iter   520 | loss 2.7212 | ppl   15.20 | lr 1.56e-04 | sparsity 0.0% | 2898.0ms/iter
Iter   530 | loss 2.7170 | ppl   15.13 | lr 1.59e-04 | sparsity 0.0% | 2891.4ms/iter
Iter   540 | loss 2.6993 | ppl   14.87 | lr 1.62e-04 | sparsity 0.0% | 2886.3ms/iter
Iter   550 | loss 2.7045 | ppl   14.95 | lr 1.65e-04 | sparsity 0.0% | 2887.5ms/iter

Eval @ iter 550: train 2.6255, val 2.6294, ppl 13.87

Generated 3 text samples
Iter   560 | loss 2.7003 | ppl   14.88 | lr 1.68e-04 | sparsity 0.0% | 17685.1ms/iter
Iter   570 | loss 2.6916 | ppl   14.76 | lr 1.71e-04 | sparsity 0.0% | 2880.4ms/iter
Iter   580 | loss 2.6900 | ppl   14.73 | lr 1.74e-04 | sparsity 0.0% | 2874.9ms/iter
Iter   590 | loss 2.6849 | ppl   14.66 | lr 1.77e-04 | sparsity 0.0% | 2873.6ms/iter
Iter   600 | loss 2.6847 | ppl   14.65 | lr 1.80e-04 | sparsity 0.0% | 2873.5ms/iter

Eval @ iter 600: train 2.6067, val 2.6109, ppl 13.61

Generated 3 text samples
Iter   610 | loss 2.6799 | ppl   14.58 | lr 1.83e-04 | sparsity 0.0% | 17675.5ms/iter
Iter   620 | loss 2.6641 | ppl   14.36 | lr 1.86e-04 | sparsity 0.0% | 2879.6ms/iter
Iter   630 | loss 2.6538 | ppl   14.21 | lr 1.89e-04 | sparsity 0.0% | 2880.4ms/iter
Iter   640 | loss 2.6658 | ppl   14.38 | lr 1.92e-04 | sparsity 0.0% | 2870.8ms/iter
Iter   650 | loss 2.6534 | ppl   14.20 | lr 1.95e-04 | sparsity 0.0% | 2873.1ms/iter

Eval @ iter 650: train 2.5716, val 2.5776, ppl 13.17

Generated 3 text samples
Iter   660 | loss 2.6499 | ppl   14.15 | lr 1.98e-04 | sparsity 0.0% | 17699.4ms/iter
Iter   670 | loss 2.6325 | ppl   13.91 | lr 2.01e-04 | sparsity 0.0% | 2878.4ms/iter
Iter   680 | loss 2.6294 | ppl   13.87 | lr 2.04e-04 | sparsity 0.0% | 2877.0ms/iter
Iter   690 | loss 2.6236 | ppl   13.79 | lr 2.07e-04 | sparsity 0.0% | 2878.9ms/iter
Iter   700 | loss 2.6158 | ppl   13.68 | lr 2.10e-04 | sparsity 0.0% | 2895.5ms/iter

Eval @ iter 700: train 2.5370, val 2.5415, ppl 12.70

Generated 3 text samples
Iter   710 | loss 2.6056 | ppl   13.54 | lr 2.13e-04 | sparsity 0.0% | 17788.5ms/iter
Iter   720 | loss 2.5976 | ppl   13.43 | lr 2.16e-04 | sparsity 0.0% | 2888.9ms/iter
Iter   730 | loss 2.5996 | ppl   13.46 | lr 2.19e-04 | sparsity 0.0% | 2898.1ms/iter
Iter   740 | loss 2.5950 | ppl   13.40 | lr 2.22e-04 | sparsity 0.0% | 2905.1ms/iter
Iter   750 | loss 2.5749 | ppl   13.13 | lr 2.25e-04 | sparsity 0.0% | 2894.4ms/iter

Eval @ iter 750: train 2.4965, val 2.5012, ppl 12.20

Generated 3 text samples
Iter   760 | loss 2.5778 | ppl   13.17 | lr 2.28e-04 | sparsity 0.0% | 17700.5ms/iter
Iter   770 | loss 2.5741 | ppl   13.12 | lr 2.31e-04 | sparsity 0.0% | 2891.9ms/iter
Iter   780 | loss 2.5566 | ppl   12.89 | lr 2.34e-04 | sparsity 0.0% | 2894.6ms/iter
Iter   790 | loss 2.5423 | ppl   12.71 | lr 2.37e-04 | sparsity 0.0% | 2896.2ms/iter
Iter   800 | loss 2.5419 | ppl   12.70 | lr 2.40e-04 | sparsity 0.0% | 2894.3ms/iter

Eval @ iter 800: train 2.4750, val 2.4834, ppl 11.98

Generated 3 text samples
Iter   810 | loss 2.5430 | ppl   12.72 | lr 2.43e-04 | sparsity 0.0% | 17705.0ms/iter
Iter   820 | loss 2.5292 | ppl   12.54 | lr 2.46e-04 | sparsity 0.0% | 2905.9ms/iter
Iter   830 | loss 2.5214 | ppl   12.45 | lr 2.49e-04 | sparsity 0.0% | 2894.5ms/iter
Iter   840 | loss 2.5222 | ppl   12.46 | lr 2.52e-04 | sparsity 0.0% | 2895.8ms/iter
Iter   850 | loss 2.5154 | ppl   12.37 | lr 2.55e-04 | sparsity 0.0% | 2897.0ms/iter

Eval @ iter 850: train 2.4346, val 2.4428, ppl 11.51

Generated 3 text samples
Iter   860 | loss 2.5035 | ppl   12.23 | lr 2.58e-04 | sparsity 0.0% | 17707.8ms/iter
Iter   870 | loss 2.5112 | ppl   12.32 | lr 2.61e-04 | sparsity 0.0% | 2895.6ms/iter
Iter   880 | loss 2.5043 | ppl   12.23 | lr 2.64e-04 | sparsity 0.0% | 2903.1ms/iter
Iter   890 | loss 2.4927 | ppl   12.09 | lr 2.67e-04 | sparsity 0.0% | 2905.4ms/iter
Iter   900 | loss 2.4885 | ppl   12.04 | lr 2.70e-04 | sparsity 0.0% | 2895.3ms/iter

Eval @ iter 900: train 2.3984, val 2.3990, ppl 11.01

Generated 3 text samples
Iter   910 | loss 2.4900 | ppl   12.06 | lr 2.73e-04 | sparsity 0.0% | 17703.5ms/iter
Iter   920 | loss 2.4826 | ppl   11.97 | lr 2.76e-04 | sparsity 0.0% | 2893.9ms/iter
Iter   930 | loss 2.4796 | ppl   11.94 | lr 2.79e-04 | sparsity 0.0% | 2893.7ms/iter
Iter   940 | loss 2.4679 | ppl   11.80 | lr 2.82e-04 | sparsity 0.0% | 2893.2ms/iter
Iter   950 | loss 2.4711 | ppl   11.84 | lr 2.85e-04 | sparsity 0.0% | 2893.4ms/iter

Eval @ iter 950: train 2.3696, val 2.3729, ppl 10.73

Generated 3 text samples
Iter   960 | loss 2.4654 | ppl   11.77 | lr 2.88e-04 | sparsity 0.0% | 17693.9ms/iter
Iter   970 | loss 2.4533 | ppl   11.63 | lr 2.91e-04 | sparsity 0.0% | 2894.2ms/iter
Iter   980 | loss 2.4454 | ppl   11.53 | lr 2.94e-04 | sparsity 0.0% | 2892.4ms/iter
Iter   990 | loss 2.4444 | ppl   11.52 | lr 2.97e-04 | sparsity 0.0% | 2891.4ms/iter
Iter  1000 | loss 2.4272 | ppl   11.33 | lr 3.00e-04 | sparsity 0.0% | 2903.3ms/iter

Eval @ iter 1000: train 2.3679, val 2.3685, ppl 10.68

--- lm-eval Benchmarks ---
Running lm-eval with limit=100 samples per task
Overwriting default num_fewshot of hellaswag from None to 0

  0%|          | 0/100 [00:00<?, ?it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 4112.34it/s]
hellaswag/acc,none: 0.2500
hellaswag/acc_stderr,none: 0.0435
hellaswag/acc_norm,none: 0.2700
hellaswag/acc_norm_stderr,none: 0.0446
Created lm-eval results table with 1 tasks

Generated 3 text samples
Saved checkpoint at iteration 1000
Iter  1010 | loss 2.4386 | ppl   11.46 | lr 3.03e-04 | sparsity 0.0% | 18974.1ms/iter
Iter  1020 | loss 2.4209 | ppl   11.26 | lr 3.06e-04 | sparsity 0.0% | 2914.1ms/iter
Iter  1030 | loss 2.3837 | ppl   10.84 | lr 3.09e-04 | sparsity 0.0% | 2897.3ms/iter
Iter  1040 | loss 2.3725 | ppl   10.72 | lr 3.12e-04 | sparsity 0.0% | 2892.4ms/iter
Iter  1050 | loss 2.3573 | ppl   10.56 | lr 3.15e-04 | sparsity 0.0% | 2884.4ms/iter

Eval @ iter 1050: train 2.2739, val 2.2785, ppl 9.76

Generated 3 text samples
Iter  1060 | loss 2.3370 | ppl   10.35 | lr 3.18e-04 | sparsity 0.0% | 17681.6ms/iter
Iter  1070 | loss 2.3471 | ppl   10.45 | lr 3.21e-04 | sparsity 0.0% | 2876.3ms/iter
Iter  1080 | loss 2.3215 | ppl   10.19 | lr 3.24e-04 | sparsity 0.0% | 2872.4ms/iter
Iter  1090 | loss 2.3180 | ppl   10.16 | lr 3.27e-04 | sparsity 0.0% | 2874.2ms/iter
Iter  1100 | loss 2.3302 | ppl   10.28 | lr 3.30e-04 | sparsity 0.0% | 2872.3ms/iter

Eval @ iter 1100: train 2.2307, val 2.2342, ppl 9.34

Generated 3 text samples
Iter  1110 | loss 2.3129 | ppl   10.10 | lr 3.33e-04 | sparsity 0.0% | 17688.4ms/iter
Iter  1120 | loss 2.2989 | ppl    9.96 | lr 3.36e-04 | sparsity 0.0% | 2874.0ms/iter
Iter  1130 | loss 2.2898 | ppl    9.87 | lr 3.39e-04 | sparsity 0.0% | 2879.7ms/iter
Iter  1140 | loss 2.2706 | ppl    9.69 | lr 3.42e-04 | sparsity 0.0% | 2877.3ms/iter
Iter  1150 | loss 2.2736 | ppl    9.71 | lr 3.45e-04 | sparsity 0.0% | 2874.0ms/iter

Eval @ iter 1150: train 2.1890, val 2.1936, ppl 8.97

Generated 3 text samples
Iter  1160 | loss 2.2675 | ppl    9.66 | lr 3.48e-04 | sparsity 0.0% | 17695.0ms/iter
Iter  1170 | loss 2.2642 | ppl    9.62 | lr 3.51e-04 | sparsity 0.0% | 2881.0ms/iter
Iter  1180 | loss 2.2606 | ppl    9.59 | lr 3.54e-04 | sparsity 0.0% | 2890.6ms/iter
Iter  1190 | loss 2.2516 | ppl    9.50 | lr 3.57e-04 | sparsity 0.0% | 2888.8ms/iter
Iter  1200 | loss 2.2532 | ppl    9.52 | lr 3.60e-04 | sparsity 0.0% | 2889.9ms/iter

Eval @ iter 1200: train 2.1701, val 2.1729, ppl 8.78

Generated 3 text samples

Reached max training time of 7200s (7201.9s elapsed)

Training complete! Total time: 120.03 minutes
Best validation perplexity achieved: 8.78
Metrics saved to /mnt/tmpfs/knlp/test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/training_metrics_stepV0.json
Saved final model: test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/final_model_stepV0.pt
* Run finished. Uploading logs to Trackio (please wait...)
Trackio tracking finished
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:       final/best_val_loss ‚ñÅ
wandb: final/best_val_perplexity ‚ñÅ
wandb:    fisher/cond_global_max ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñá‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñá‚ñÉ
wandb:   fisher/cond_global_mean ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÖ
wandb:  fisher/eigmax_global_max ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñá‚ñÇ
wandb: fisher/eigmax_global_mean ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÖ
wandb:  fisher/eigmax_global_min ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ
wandb:  fisher/layer0/eigmax_max ‚ñÉ‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÅ‚ñá‚ñÑ
wandb: fisher/layer0/eigmax_mean ‚ñÑ‚ñÉ‚ñÅ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÖ
wandb:  fisher/layer0/head0/cond ‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñà‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñà
wandb:                      +502 ...
wandb: 
wandb: Run summary:
wandb:       final/best_val_loss 2.17287
wandb: final/best_val_perplexity 8.7835
wandb:    fisher/cond_global_max 4778174.5
wandb:   fisher/cond_global_mean 3453172.76389
wandb:  fisher/eigmax_global_max 0.04834
wandb: fisher/eigmax_global_mean 0.03569
wandb:  fisher/eigmax_global_min 0.02162
wandb:  fisher/layer0/eigmax_max 0.04778
wandb: fisher/layer0/eigmax_mean 0.03683
wandb:  fisher/layer0/head0/cond 4127787.75
wandb:                      +502 ...
wandb: 
wandb: üöÄ View run gpt2_adamwspam_mla_kv at: https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-h100/runs/vzaax44e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/mcgrof-citizen/gpt2-kvsplice-ablation-h100
wandb: Synced 5 W&B file(s), 27 media file(s), 54 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251128_112758-vzaax44e/logs
W&B tracking finished

Training complete!
[W1128 13:28:12.030368656 AllocatorConfig.cpp:29] Warning: PYTORCH_HIP_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
2025-11-28 13:28:12,784 - INFO - Training completed successfully
2025-11-28 13:28:13,508 - INFO - Simple GPU monitoring completed
2025-11-28 13:28:14,496 - INFO - GPU performance graphs generated: test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/gpu_stats_gpt2_adamwspam_mla_kv_20251128_112752_plot.png
2025-11-28 13:28:14,497 - INFO - GPU stats saved to: test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/gpu_stats_gpt2_adamwspam_mla_kv_20251128_112752.json
2025-11-28 13:28:14,498 - INFO - Generating performance graphs...
2025-11-28 13:28:15,486 - INFO - GPU performance graphs saved to test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/gpu_stats_gpt2_adamwspam_mla_kv_20251128_112752_plot.png
2025-11-28 13:28:15,486 - INFO - Performance graphs saved to: test_matrix_results_20251128_092718/gpt2_adamwspam_mla_kv/gpu_stats_gpt2_adamwspam_mla_kv_20251128_112752_plot.png
2025-11-28 13:28:15,486 - INFO - Training with monitoring completed successfully!
[W1128 13:28:15.431812354 AllocatorConfig.cpp:29] Warning: PYTORCH_HIP_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
